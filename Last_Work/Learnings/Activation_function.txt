# Sigmoid Function
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x):
    s = 1/(1+np.exp(-x))
    ds = s*(1-s)
    return s, ds

x = np.arange(-6,6,0.01)
sigmoid(x)
fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x,sigmoid(x)[0], color="#307EC7", linewidth=3, label="sigmoid")
ax.plot(x,sigmoid(x)[1], color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()


# TanH fucntion
import matplotlib.pyplot as plt
import numpy as np

def tanh(x):
    t = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
    dt = 1-t**2
    return t, dt

z = np.arange(-4,4,0.01)
tanh(z)[0].size, tanh(z)[1].size
fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position('center')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(z,tanh(z)[0], color="#307EC7", linewidth=3, label="tanh")
ax.plot(z,tanh(z)[1], color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()



# relu activation and its derivative
import matplotlib.pyplot as plt
import numpy as np

def relu(x):
    r = np.maximum(0, x)
    dr = np.where(x <= 0, 0, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
relu_values, relu_derivatives = relu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, relu_values, color="#307EC7", linewidth=3, label="ReLU")
ax.plot(x,relu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()



# Leaky ReLU
import matplotlib.pyplot as plt
import numpy as np

def leaky_relu(x, alpha=0.01):
    r = np.maximum(alpha*x, x)
    dr = np.where(x < 0, alpha, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
leaky_relu_values, leaky_relu_derivatives = leaky_relu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, leaky_relu_values, color="#307EC7", linewidth=1, label="Leaky ReLU")
ax.plot(x,leaky_relu_derivatives, color="#9621E2", linewidth=1, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()



# Parametric Rectified Linear Unit (PReLU)
import matplotlib.pyplot as plt
import numpy as np

def prelu(x, alpha=0.25):
    r = np.maximum(alpha*x, x)
    dr = np.where(x < 0, alpha, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
prelu_values, prelu_derivatives = prelu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, prelu_values, color="#307EC7", linewidth=3, label="PReLU")
ax.plot(x, prelu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()



# Exponential Linear Unit (ELU)
import matplotlib.pyplot as plt
import numpy as np

def elu(x, alpha=1.0):
    r = np.where(x >= 0, x, alpha*(np.exp(x)-1))
    dr = np.where(x >= 0, 1, alpha*np.exp(x))
    return r, dr

x = np.arange(-6, 6, 0.01)
elu_values, elu_derivatives = elu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, elu_values, color="#307EC7", linewidth=3, label="ELU")
ax.plot(x, elu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()
# SoftPlus
import matplotlib.pyplot as plt
import numpy as np

def softplus(x):
    r = np.log(1+np.exp(x))
    dr = 1/(1+np.exp(-x))
    return r, dr

x = np.arange(-6, 6, 0.01)
softplus_values, softplus_derivatives = softplus(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, softplus_values, color="#307EC7", linewidth=3, label="Softplus")
ax.plot(x, softplus_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()



# ArcTan
import matplotlib.pyplot as plt
import numpy as np

def arctan(x):
    r = np.arctan(x)
    dr = 1/(1+x**2)
    return r, dr

x = np.arange(-6, 6, 0.01)
arctan_values, arctan_derivatives = arctan(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, arctan_values, color="#307EC7", linewidth=3, label="Arctan")
ax.plot(x, arctan_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()



# TanH
import matplotlib.pyplot as plt
import numpy as np

def tanh(x):
    r = np.tanh(x)
    dr = 1 - r**2
    return r, dr

x = np.arange(-6, 6, 0.01)
tanh_values, tanh_derivatives = tanh(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, tanh_values, color="#307EC7", linewidth=3, label="Tanh")
ax.plot(x, tanh_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

----------------------------------------------------------------------------------

#extra part for 1st prac - after activation functions
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

X, y =make_moons(n_samples=1000, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
scalar = StandardScaler()
X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)

model = Sequential([
Dense(10, input_shape=(2,), activation='tanh'),
Dense(10, activation='elu'),
Dense(1, activation='sigmoid')])
model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=1)
def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min()- 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min()- 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z =model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z =Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Decision Boundary and Test Data Points')
plt.figure(figsize=(10, 6))
plot_decision_boundary(model, np.vstack((X_train, X_test)), np.hstack((y_train, y_test)))
plt.show()
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Training Performance')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()
